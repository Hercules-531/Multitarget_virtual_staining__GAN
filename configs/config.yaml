# MultiStain-GAN Configuration
# Optimized for NVIDIA H100 40GB GPU

# Model Architecture
model:
  image_size: 256
  latent_dim: 64 # INCREASED from 16 - more diversity capacity
  style_dim: 256 # INCREASED from 64 - better domain separation (CRITICAL!)
  num_domains: 4 # ER, PR, Ki67, HER2

  generator:
    enc_channels: [64, 128, 256, 512]
    num_res_blocks: 8 # Increased for better quality
    vit_blocks: 6 # More ViT blocks for global context
    vit_heads: 8
    use_pretrained: true
    use_checkpoint: false # Disable - H100 has enough VRAM

  discriminator:
    num_scales: 2 # Reduced from 3 to prevent too-small feature maps
    base_channels: 64
    num_layers: 3 # Reduced from 5 - prevents 4x4 kernel issues
    use_spectral_norm: true

  style_encoder:
    pretrained: true
    freeze_layers: 2

  mapping_network:
    num_layers: 6 # Deeper mapping network
    hidden_dim: 512

# Dataset
data:
  root_dir: "D:/IHC4BC (IHC for Breast Cancer) - Compressed Dataset/archive (4)/IHC4BC_Compressed"
  domains: ["ER", "PR", "Ki67", "Her2"]
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  num_workers: 8 # More workers for faster data loading
  pin_memory: true
  paired_only: true
  paired_reference: true
  balanced_sampling: true # NEW: Balance samples across domains to prevent collapse

# Training - Stability-focused configuration
training:
  batch_size: 12 # Higher batch for paired fidelity on H100
  accumulate_grad: 4 # Effective batch = 48
  epochs: 300

  optimizer:
    type: AdamW
    lr_generator: 1.0e-4 # Higher G learning rate to catch up with D
    lr_discriminator: 5.0e-5 # Lower D learning rate to give G a chance
    beta1: 0.5 # CHANGED from 0.0 - adds momentum for stability
    beta2: 0.999 # CHANGED from 0.99 - smoother second moment
    weight_decay: 1.0e-5 # Reduced for generator to explore more

  # Training stability  
  grad_clip_max_norm: 1.0 # Slightly looser for G gradient flow
  noise_std: 0.1 # Inject noise to D inputs for regularization

  scheduler:
    type: CosineAnnealingLR
    T_max: 100
    eta_min: 1.0e-6

  # Loss weights - REBALANCED to fix mode collapse
  losses:
    adversarial: 2.0 # INCREASED - stronger signal to generator
    cycle: 5.0 # REDUCED from 15 - was causing averaging/blur
    style_reconstruction: 1.0
    style_diversification: 1.0 # INCREASED from 0.1 - encourage diversity
    contrastive_nce: 1.0 # Reduced slightly
    perceptual: 2.0 # INCREASED - better feature matching
    ssim: 0.5 # Reduced - can conflict with style transfer
    l1_reconstruction: 5.0 # Direct color matching with target
    gradient_matching: 3.0 # Edge preservation
    sharpness_matching: 2.0 # Prevent blur
    nce_temperature: 0.07 # Standard temperature
    nce_logit_clip: 50.0
    ssim_eps: 1.0e-6

  # Regularization - adjusted for better G/D balance
  r1_gamma: 10.0 # INCREASED - regularize D more to help G
  r1_interval: 4 # MORE FREQUENT - apply R1 every 4 steps
  ema_decay: 0.999

# Logging
logging:
  log_dir: "./logs"
  checkpoint_dir: "./checkpoints"
  sample_dir: "./samples"
  log_every: 100
  save_every: 5000
  sample_every: 500 # More frequent samples

# Evaluation
evaluation:
  metrics: ["fid", "ssim", "lpips"]
  num_samples: 1000

# Hardware & Optimization - H100 Specific
device: "cuda"
seed: 42
mixed_precision: true # BF16 on H100 is excellent
compile_model: true # torch.compile for max speed
cudnn_benchmark: true
